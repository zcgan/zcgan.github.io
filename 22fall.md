---
layout: 22fall
title: Teaching
description: Mathematical Methods
---


## Description
Machine learning has received a lot of hype over the last decade, with techniques such as convolutional neural networks and deep learning powering a new generation of data-driven analytics. On the other hand, material science has benefited a lot from large-scale modeling & simulation through Molecular Dynamics, Density Functional Theory, and Differential Equations describing rigorous scientific laws. This course aims to provide students trainings with a convergence of the two disciplines. We will start from machine learning basics, its mathematical foundations, then move on to modern machine learning methods for material science problems and hands-on study with Python. Particularly, students will learn about how to combine the data-driven ML techniques with existing knowledge of material science to give reliable physical predictions. Various case studies will be discussed, with real-world material science applications.

## Lectures
Tuesday and Thursday, 3:00–4:30pm, remotely via Canvas -> Zoom


### Lecture slides (available upon request)

<ol style="margin-left:12px;">
    <li> Introduction on Machine Learning </li>
    <li> Mathematical Preliminaries </li>
    <li> TensorFlow, Fourier Analysis and Nyquist Sampling Thm </li>
    <li> PyTorch, Over/Under Fitting </li>
    <li> Convolutional Neural Network (CNN) </li>
    <li> Recurrent Neural Network (RNN) </li>
    <li> Ensemble Learning for Materials Feature Prediction </li>
    <li> GAN, ResNet and GCN </li>
    <li> Mathematical Theory and Scientific Applications </li>
    <li> Deep Learning for Partial Differential Equations </li>
    <li> Physics-Informed Machine Learning (PINN) and DeepXDE </li>
    <li> Physics Inspired Machine Learning </li>
    <li> Support Vector Machine (SVM) and kernel methods </li>
    <li> Dimension Reduction and Metric Learning for High-dimensional data </li>
    <li> Clustering Techniques and Applications to 2D Ising Model </li>
    <li> Reinforcement Learning </li>
</ol>

<!--<ol style="margin-left:12px;">
    <li> <a href="nsopt/slides/introduction.pdf">Introduction</a></li>
    <li> <a href="nsopt/slides/lecture-01.pdf">Gradient method</a></li>
    <li> <a href="nsopt/slides/lecture-02.pdf">Proximal gradient method</a></li>
    <li> <a href="nsopt/slides/lecture-03.pdf">Krasnosel'skii-Mann iteration</a></li>
    <li> <a href="nsopt/slides/lecture-04.pdf">Backward--Backward splitting</a></li>
    <li> <a href="nsopt/slides/lecture-05.pdf">Douglas--Rachford splitting</a></li>
    <li> <a href="nsopt/slides/lecture-06.pdf">Primal--Dual splitting</a></li>
    <li> <a href="nsopt/slides/lecture-07.pdf">Other operator splitting methods</a></li>
    <li> <a href="nsopt/slides/lecture-08.pdf">Alternating direction method of multipliers</a></li>
    <li> <a href="nsopt/slides/lecture-09.pdf">Non-convex optimisation</a></li>
    <li> <a href="nsopt/slides/lecture-10.pdf">Stochastic optimisation</a></li>
</ol>-->

<!-- **Acknowledgement:** some slides are based on the lecture slides of [Prof. Stephen Boyd](https://web.stanford.edu/~boyd/) and [Prof. Lieven Vandenberghe](http://www.seas.ucla.edu/~vandenbe/).
-->

<!--### Projects
- [Project 1](nsopt/projects/project-01.pdf) Comparison of gradient descent, heavy-ball method and Nesterov's acceleration scheme, and their proximal versions. ([Instructions](nsopt/project1), [data](nsopt/projects/data.zip), [MATLAB code](nsopt/projects/src_Project1.zip))    
- [Project 2](nsopt/projects/project-02.pdf) Principal component pursuit. ([Instructions](nsopt/project2), [MATLAB code](nsopt/projects/src_Project2.zip)) 
-->

<br>

#### References
- N. Thuerey, P. Holl, M. Mueller, P. Schnell, F. Trost, K. Um. Physics-based Deep Learning. Freely Available at physicsbaseddeeplearning.org
- I. Goodfellow, Y. Bengio, A. Courville. Deep Learning. The MIT Press, 2016.
- Koki Saitoh (translated by Yujie Lu). Deep Learning from Scratch (in Chinese). O’Reilly Japan, Inc.
- Zhihua Zhou. Machine Learning (in Chinese). The Tsinghua Press, 2016.


